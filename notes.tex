\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{geometry}

\geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\begin{document}
\title{MTHE 474 Notes}
\author{Timothy Liu}
\date{Fall 2022}
\maketitle
\newpage

\tableofcontents

\newpage
\section{Information Measures (Weeks 1-3)}
\subsection{Information Measures for Discrete Systems}
\subsubsection{Definitions}
\begin{flushleft}
    \begin{itemize}
   
    \item \textbf{Definition 2.2:} Entropy of discrete random variable \(X\) with pmf \(P_X(*)\) is defined as \[H(X):=-\sum_{x \in X}{P_X(x)*\log_{2}{P_X(x)}}\]

    \item \textbf{Definition 2.2:} Entropy of discrete random variable \(X\) with pmf \(P_X(*)\) is defined as \[H(X):=-\sum_{x \in X}{P_X(x)*\log_{2}{P_X(x)}}\]
    \item \textbf{Definition 2.8 (Joint entropy):} \[H(X,Y) := - \sum_{(x,y) \in \mathcal{X} \times \mathcal{Y}}{P_{X,Y}{(x,y)}*\log_{2}{P_{({X,Y})}{(x,y)}}}\]
    \item \textbf{Definition 2.9 (Conditional entropy):} \[H(Y|X) := \sum_{x \in \mathcal{X}} P_X(x) (-\sum_{y \in \mathcal{Y}} P_{Y|X}{(y|x)} * \log_{2}{P_{Y|X}{(y|x)}})\]
    \item 
    \end{itemize}
\end{flushleft}



\subsubsection{Lemmas/Theorems}
\begin{flushleft}
    \begin{itemize}
        \item \textbf{Lemma 2.4 (Fundamental Inequality):} \(\forall\) \(x>0\) and \(D > 1\) we have \[\log_{D}{(x)} \leq \log_{D}{e}*(x-1) \]

        \item \textbf{Lemma 2.5 (Non-negativity):} \(H(X) \geq 0\)
    
        \item \textbf{Lemma 2.6 (Entropy Upper-Bound):} \(H(X) \leq \log_{2}{|\mathcal{X}|} \) where random variable X takes values from finite set \(\mathcal{X}\)
    
        \item \textbf{Lemma 2.7 (Log-Sum inequality):} For nonnegative numbers, \(a_1, a_2, \ldots, a_n\) and \(b_1, b_2, \ldots, b_n\)
        \[\sum_{i=1}^{n}(a_i \log_D \frac{a_i}{b_i}) \geq (\sum_{i=1}^n a_i) \log_D \frac{\sum_{i=1}^n a_i}{\sum_{i=1}^n b_i}\]  
        with equality iff for all \(i=1, \ldots, n\)
        \[\frac{a_i}{b_i} = \frac{\sum_{j=1}^n a_j}{\sum_{j=1}^n b_j}\]
        is constand and does not depend on i
        \item \textbf{Theorem 2.10 (Chain rule for entropy): } \( H(X,Y) = H(X) + H(Y|X)\)
        \item \textbf{Theorem 2.12 (Conditioning never increases entropy):} \(H(X|Y) \leq H(X)\) \\
        with equality holding iff \(X\) and \(Y\) are independent
        \item \textbf{Lemma 2.13 (Entropy is additive for independent RVs): } For independent \(X, Y\)
        \[H(X,Y) = H(X) + H(Y)\]
        \item \textbf{Lemma 2.14 (Conditional entropy is lower additive): } \(H(X_1, X_2|Y_1, Y_2) \leq H(X_1|Y_1) + H(X_2|Y_2)\)
        \\ with equality holding iff 
        \[P_{X_1, X_2|Y_1, Y_2} (x_1, x_2|y_1, y_2) = P_{X_1|Y_1}(x_1|y_1)P_{X_2|Y_2}(x_2|y_2)\]
        for all \(x_1, x_2, y_1, y_2\)

    \end{itemize}
\end{flushleft}

\subsection{Mutual Information}

\begin{flushleft}
    \subsubsection{Definitions}
    \begin{itemize}
        \item \textbf{Definition 2.2.1 (Mutual Information):} \[I(X;Y):= H(X) - H(X|Y)\]
        \item \textbf{Definition 2.2.2 (Conditional Mutual Information):} \[I(X;Y|Z) := H(X|Z) - H(X|Y, Z)\]
    \end{itemize}
    \subsubsection{Lemmas}
    \begin{itemize}
        \item \textbf{Lemma 2.15 (Properties of Mutual Information): } 
        \begin{flalign}
        &1.\ I(X;Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P_{X,Y}(x,y) \log_2{\frac{P_{X,Y}{(x,y)}}{P_X{(x)}P_Y{(y)}}} \\ 
        &2.\ I(X;Y) = I(Y;X) = H(Y) - H(Y|X) \\
        &3.\ I(X;Y) = H(X) + H(Y) - H(X,Y) \ \  \\
        &4.\ I(X;Y) \leq H(X) \ \ \text{equality iff X is a function of Y} \\
        &5.\ I(X;Y) \leq 0 \ \text{with equality iff X and Y are independent} \\
        &6.\ I(X;Y) \leq \min\{\log_2{{|\mathcal{X}}|}, \log_2{|\mathcal{Y}}|\}
        \end{flalign}
        \item \textbf{Lemma 2.16 (Chain Rule for Mutual Information):} \[I(X; Y, Z) = I(X;Y) + I(X; Z|Y) = I(X;Z) + I(X;Y|Z)\]
        \item \textbf{Theorem 2.17 (Chain Rule for entropy):} \(X^n := (X_1, \ldots, X_n)\) and \(x^n := (x_1, \ldots, x_n)\)
        \[H(X^n) = \sum_{i=1}^{n}{H(X_i|X^{i-1})}\]
        \item \textbf{Theorem 2.18 (Chain Rule for conditional entropy):}
        \[H(X_1, X_2, \ldots, X_n | Y) = \sum_{i=1}^{n} H(X_i|X_{i-1}, \ldots, X_1, Y)\]
        \item \textbf{Theorem 2.19 (Chain Rule for Mutual information):}
        \[I(X_1, X_2, \ldots, X_n; Y) = \sum_{i=1}^{n} I(X_i; Y|X_{i-1}, \ldots, X_1)\]
        Where \(I(X_i;Y|X_{i-1}, \ldots, X_1):=I(X_1, Y)\) for \(i = 1\)
    \end{itemize}
\end{flushleft}

\subsection{Conditional Divergence}
\subsubsection{Definitions}
\begin{itemize}
    \item \textbf{Definition 2.29 (Divergence):} Given 2 discrete random variables \(X\) and \(\hat{x}\) defined over common alphabet \(\mathcal{X}\) divergence is defined by,
    \[D(X||\hat{X}) := E_x[\log_2 \frac{P_X (X)}{P_{\hat{X}}(X)}] = \sum_{x \in \mathcal{X}} P_X(x) \log_2 \frac{P_X(x)}{P_{\hat{X}}(x)}\]
\end{itemize}
\subsubsection{Theorems}
\begin{itemize}
    \item \textbf{Lemma 2.30 (Nonnegativity of Divergence):} \(D(X||\hat{X}) \geq 0\), with equality iff \(P_X(x) = P_{\hat{X}}(x)\) for all \(x \in \mathcal{X}\)
\end{itemize}
% To do later

\subsection{Fano's Inequality}
\subsubsection{Definitions}
\subsubsection{Theorems/Lemmas}
\begin{itemize}
    \item \textbf{Lemma 2.6 (Fano's inequality):} Let \(X\) and \(Y\) be two random variables with alphabets \(\mathcal{X} \text{ and } \mathcal{Y}\) respectively (\(\mathcal{X}\) is finite but \(\mathcal{Y}\) can be countably infinite).
    Let \(\hat{X} := g(Y)\) represent the estimate of X by observing Y and \(P_e := \Pr[\hat{X} \neq X]\) represent the probability of error of this observation
    \\Then the following holds
    \[H(X|Y) \leq h_b(P_e) + P_e * \log_{2} (\mid \mathcal{X} \mid -1)\]
    Where \(h_b(P_e)\) is the binary entropy with probability \(P_e\)
\end{itemize}

\subsection{Data Processing Inequality}
\subsubsection{Definitions}
\begin{itemize}
    \item \textbf{Lecture 7 Definition (Markov Chain):} Three jointly distributed random variables \(X,Y,Z\) are said to form a Markov Chain (in that order), denoted by \(X \to Y \to Z\)
    \\if:
    \[P_{XZ|Y}(x,z|y) = P_{X|Y}(x|y) P_{Z|Y}(z|y) \Longleftrightarrow P_{Z|XY}(z|x,y) = P_{Z|Y}(z|y)\]
    \\ \(\forall x \in X, y \in Y, z \in Z\)
    \\
    \begin{itemize}
        \item The probability of each event ONLY depends on the state attained on the previous event
    \end{itemize}
\end{itemize}
\subsubsection{Theorems}
\begin{itemize}
    \item \textbf{Lecture 7 Theorem (Data Processing Inequality):} If \(X \to Y \to Z\), then 
    \[I(X;Y) \leq I(X;Z)\]
    \begin{itemize}
        \item Another way to think of this is that the futher the RVs are along the markov chain, the less relevant the RVs are with eachother and the less information we get
    \end{itemize}
    \item \textbf{Lecture 8 Theorem (DPI for Divergence):} Given fixed conditional PMF \(P_{Y|X}\) on \(y \times x\), which describes a channel with input x and output y,
    let \(P_x\) and \(q_x\) be 2 possible PMFs for input \(x\) with corresponding output PMFs \(P_y \text{ and } q_y\) respectively, then
    \[D(P_x||q_x) \leq D(P_y||q_y)\]
\end{itemize}

\subsection{Convex/Concavity of Information Measures}
\subsubsection{Definitions}
\begin{itemize}
    \item \textbf{Lecture 6 Definition (Convex Set):} \[\text{A subset K of } \mathbb{R} \text{ is called convex if the line segment joining any two points in K also lies in K}\]
    \item \textbf{Lecture 6 Definition (Convex Function):} The function \(f: k \to \mathbb{R}\) where k is a convex subset of \(\mathbb{R}^n\), is called convex on \(k\) if \(\forall x_1,x_2 \in k\) and \(\lambda \in [0,1]\),
    \[f(\lambda x_1 + (1-\lambda)x_2)\leq \lambda f(x_1) + (1-\lambda) f(x_2)\]
    Strict equality holds whenever \(x_1 \neq x_2\) and \(0< \lambda < 1\) then \(f\) is called strictly convex
    \item \textbf{Lecture 6 Definition (Concave Function):} \(f: k \to \mathbb{R}\) is concave on \(k\) (where \( k \subseteq \mathbb{R}^n\) is a concave subset) if \(-f\) is convex. In other words:
    if \(\forall x_1,x_2 \in k\) and \(\lambda \in [0,1]\),
    \[f(\lambda x_1 + (1-\lambda)x_2)\geq \lambda f(x_1) + (1-\lambda) f(x_2)\]

\end{itemize}
\subsubsection{Theorems}
\begin{itemize}
    \item \textbf{Lecture 6 Theorem (Jensen's Inequality):} Let \(K \subseteq \mathbb{R}\) (where \(K\) is a convex set?) and let \(f:k \to \mathbb{R}\) be a convex function. Also let  x be a RV with alphabet \(\mathcal{X} \subseteq k\) and finite mean, then
    \[E[f(x)] \leq f(E[x])\]
    Also if f is strictly convex, then the inequality is strict unless x is deterministic
    \item \textbf{Lecture 7 Theorem (Convexity/Concavity of Information Measures):}
       \\i. \(D(p||q)\) \ is convex in the pair \((p,q)\) (ie: if \(p_1,q_1\) and \(p_2,q_2\) are two pairs of PMFs defined on \(\mathcal{X}\)) then: \\
        \[D(\lambda p_1 + (1-\lambda)p_2 || \lambda q_1 + (1-\lambda) q_2) \leq \lambda D(p_1 || q_1) + (1- \lambda) D(p_2 || q_2)\]
        \(\forall \lambda \in [0,1]\)
        \\
        \\
        ii.\ if \(x \sim  P_x\), then
        \[H(x) = H(p_x) \text{ is concave in } P_x\]
        \\
        iii.\ If \((x,y)~P_X P_{Y|X}\), then \(I(X;Y) = I(P_X, P_{Y|X})\) is concave in \(P_X\) for fixed \(P_{Y|X}\) and convex in \(P_{Y|X}\) for fixed \(P_X\)

\end{itemize}

\section{Chapter 3 Topics}
\subsection{Principles of Data Compression (Week 4)}
\subsubsection{Definitions}
\begin{itemize}
    \item \textbf{Lecture 9 Definition (Discrete Memoryless Source):}
    A DMS is an infinite sequence of i.d.d random variables \(\{X_{i}\}^\infty_{i=1} = \{X_1, X_2, \ldots\}\), such that all the random variables have a common PMF \(P_x\) defined on the alphabet/finite set \(\mathcal{X}\)
\\ i.d.d property: \(P(X_1 = a_1, \ldots, X_n = a_n) = \prod_{i=1}^n P(X_i=a_i)\)
    \item \textbf{Lecture 9 Definition (Convergence in Probability):} Given sequence \(\{x_i\}^\infty_{i=1}\) of RVs and RV Z,
    \[X_n \xrightarrow[]{n\to\infty} \text{ in probability} \Longleftrightarrow \forall \epsilon > 0, \lim_{n\to\infty} P(|X_n-Z|>\epsilon)=0\]    
    
    \item \textbf{Lecture 9 Definition (Typical Set):} For a DMS \(\{X_i\}^\infty_{i=1}\) with PMF \(P_x\) and entropy \(H(X)\), given integer \(n \geq 1\)
    and \(\epsilon >\) 0, the typical set \(A_\epsilon^{(n)}\) with respect to the source is
    \[A^{(n)}_\epsilon = \{a^n \in \mathcal{X}: \left |-\frac{1}{n} \log_2 P_{X^n}(a^n) - H(X) \right|\leq\epsilon\}\]
    \[A^{(n)}_\epsilon = \{a^n \in \mathcal{X}: 2^{-n*(H(X) + \epsilon)} \leq P_{X^n}(a^n) \leq 2^{-n*(H(X) - \epsilon)}\]


    \item \textbf{Lecture 9 Definition (Code block):} Given integers \(D \geq 2\), \(n \geq 1\) and \(k = k(n)\) (k is a function of n and describes number of symbols in a block)  a \((k,n)\) D-ary Fixed length code \(\rho\) for a DMS \(\{X_{i}\}^\infty_{i=1}\) with alphabet \(\mathcal{X}\) consists of the following pair of encoding and decoding functions
    \[\text{Encoding: } f: \mathcal{X}^n \to \{0, 1, \ldots, D-1\}^k\] 
    \[\text{Decoding: } g: \{0, 1, \ldots, D-1\}^k \to \mathcal{X}\]
    The range of f is called the \(\mathit{codebook}\)
    \\ The code (or Compression) rate is defined as \(R = \frac{k}{n}\) in D-ary code symbols / Source symbols 
   \\
    \\(Note: \(\{a,b,c\}^k\) denotes the cartesian product of the set \(\{a,b,c\}\) k times)
    \\
   \\ k denotes the length of output source
   \\ D represents number of code symbols in the code (output) alphabet
   \\ n represents the length of input source
   \\ \(|\mathcal{X}|\) represents the number of code symbols in the source (input) alphabet
   
    \item \textbf{Lecture 9 Definition (Probability of Decoding Error): } Measures the code's reliability and defined as
   \[P_e := P(g(f(x^n))) \neq x^n\]
   Predicament is that we want code to be efficient and reliable (ie code rate as small as possible and probability of error is also as small as possible)
   
   \item \textbf{Lecture 9 Definition (Lossless):} A \((k,n)\) D-ary code for the source is called uniquely decodable or lessless if
    \[f: \mathcal{X}^n \to \{0,1,\ldots, D-1\}\]
    is an invertable map and \(g = f^{-1}\)
    \item \textbf{Lecture 11 Definition (Stationary): } The source \(\{X_i\}^\infty_{i=1}\) is called stationary if
    \[P(X_1=a_1, X_2 = a_2, \ldots, X_n = a_n) = P(X_{1+z} = a_1, X_{2+z}, \ldots X_{n+z} = a_n)\]
    \\ \(\forall a^n = (a_1, \ldots, a_n) \in \mathcal{X}^n \text{ and integers } n, z \geq 1\)
    \\ Stating that the joint distribution is invariant to time shifts
\end{itemize}
\subsubsection{Theorems/Lemmas}
\begin{itemize}
    \item \textbf{Lecture 9 Theorem (Weak Law of Large Numbers):} if \(\{x_i\}^{\infty}_{i=i}\) is a DMS then 
\[\frac{1}{n} \sum^n_{i=1} x_i \xrightarrow{n\to\infty} E[X]\] in probability
    \item \textbf{Lecture 9 Theorem (Asymptotic Equipartition Property):} (also known as ``entropy stability property``)
    For a DMS \(\{X_i\}^\infty_{i=1}\) with PMF \(P_x\) and alphabet \(\mathcal{X}\),
    \[-\frac{1}{n} \log_2 P_{X^n}(x^n) \xrightarrow[]{n\to\infty}H(X) \text{ in probability}\]
    \item \textbf{Lecture 9 Theorem (Consequence of AEP):} For a DMS \(\{X_i\}_{i=1}^\infty\) with PMF \(P_x\) and entropy \(H(X)\) the typical set satisfies
    \begin{itemize}
        \item \(\lim_{n\to\infty} P(A_{\epsilon}^{(n)})=1\)
        \item \(|A_\epsilon^{(n)}| \leq 2^{n(H(X)+\epsilon)}\) Where \(|A|\) is the size of set A
        \item \(|A_\epsilon^{(n)}| \geq (1-\epsilon)2^{n(H(X)-\epsilon)}\) for \(n\) sufficiently large
    \end{itemize}
    \item \textbf{Lecture 10 Theorem (Shannon's Fixed-length lossless source coding theorem for DMS):} For integer \(D\leq2\), consider a DMS \(\{X_i\}^{\infty}_{i=i}\) with alphabet \(\mathcal{X}\), PMF \(P_x\), \\and
    source entropy \(H_D(X) = - \sum_{a \in \mathcal{X}} P_X(a) \log_D P_X(a)\) then the following hold:
    \begin{itemize}
        \item (i.\ forward part) \(\forall \epsilon \in (0,1)\) and \(0<\delta<\epsilon\), \(\exists\) a sequences of D-ary (k,n) fixed length codes \(\rho_n\) such that,
        \[\limsup_{n\to\infty}\frac{k}{n} \leq H_d(x) + \delta\] 
        \[\text{ and } \]
        \[P_e(\rho_n)< \epsilon \text{ for n sufficiently large}\]
        
        \item (ii.\ strong converse part) \(\forall \epsilon \in (0,1)\) and any sequence of D-ary (k,n) fixed-length codes \(\rho_n\) for the source with \(\limsup_{n\to\infty}\frac{k}{n}<H_D(X)\), we have
        \[P_e(\rho_n)>1-\epsilon \text{ for n sufficiently large}\]
    \end{itemize}
    \textbf{Consequence from this theorem:}
    \[H_D(x) = \inf \{R: \text{R achieveable}\}\]
    where
    

    % Why is the converse part different from the strong converse part? weaker cuz the stronger implies the weaker, but not the other way around?
    %continue this part later
    \begin{align*}
        \text{R achieveable} \Longleftrightarrow & \forall \epsilon>0, \exists \text{ D-ary (k,n) fixed length codes } \rho_n \text{ such that } \limsup_{n \to\infty}\frac{k}{n}\leq R \\
         & \text{ and } P_e(\rho_n)<\epsilon \text{ for n sufficiently large}
    \end{align*}

    \item \textbf{Lecture 11 Lemma:} if source \(\{X_i\}_{i=1}^\infty\) is stationary, the it is i.d.d
    \item \textbf{Lecture 11 Lemma:} A DMS (i.d.d) source is stationary
\end{itemize}
\subsection{Sources with Memory and Markov Chains (Weeks 4 and 5)}
\subsubsection{Definitions}
\begin{itemize}
    \item \textbf{Lecture 11 Definition (Markov chain and process):} A source \(\{X_i\}_{i=1}^\infty\) with finite alphabet \(\mathcal{X}\) is called a markov chain on markov process if \(\forall i = 1, 2, \ldots\)
    \[P(X_i=a_i | X^{i-1} = a^{i-1}) = P(X_i = a_i | X_{i-1} = a_{i-1})\]
    \\ \(\forall a^i = (a_1, \ldots, a_{i-1}) \in \mathcal{X}^i\)
    \\ \\
    SIDENOTE: If \(\{X_i\}_{i=1}^\infty\) is a MC, then its n-fold PMF can be written as \\
    \[P_{X^n}(a^n) = P_{X^1}(a_1) \prod_{i=2}^n P(X_i=a_i|X_{i-1}=a_{i-1})\]
    
    \item \textbf{Lecture 11 Definition (M'th order Markov Chain):} \(\{X_i\}_{i=1}^\infty\) is called a Markov Source of memory M, where \(M\geq 1\) fixed integer, if
    
    \[P(X_i=a_i|X^{i-1} = a^{i-1}) =P(X_i=a_i|X_{i-1} = a_{i-1}, \ldots, X_{i-M} = a_{i-M})\]
    \(\forall i>M, a^i \in \mathcal{X}^i\)
    \\ \\ (current state is dependent on the previous M states)
    \item \textbf{Lecture 11 Definition (Various Notations):}
    \begin{itemize}
        \item For a markov chain \(\{X_i\}^\infty_{i=1}\), \(X_i\) is called the state of the MC at time i
        \item A markov chain \(\{X_i\}^\infty_{i=1}\) is called \textit{time-invarient} or \textit{homogeneous} if its conditional PMFs \(P_{X_i|X_{i-1}}\) is not dependent on time i
        \\ ie: \(P(X_i = b| X_{i-1} = a) = P(X_2=b|X_1=a)\)  \(\forall i\geq 2, \forall a,b \in \mathcal{X}\)
    \end{itemize}
    \item \textbf{Lecture 11 Definition (Irreducible):} a MC is called \textit{irreducible} if one can go from any state value in \(\mathcal{X}\) to any other state value in \(\mathcal{X}\) in a finite number of transitions with positive probabilities. (ie: no closed loops)
    \item \textbf{Lecture 11 Definition (Stationary Distribution):} For a MC with alphabet \(\mathcal{X}\) of size \(|\mathcal{X}|=M\) and transition matrix Q (of size \(M \times M\)), a distribution \(\Pi\) on \(\mathcal{X}\) is called a \textit{stationary distribution}
    for the MC if \(\forall a \in \mathcal{X},\)
    \[\Pi (a) = \sum_{b \in \mathcal{X}} \Pi(b) P_{ab}\]
    \\ where \(P_{ab}\) is the (a,b) element in the transition probability matrix and is equal to \(P_{X_2|X_1}(b|a)\)

    
\end{itemize}
\subsubsection{Theorems/Lemmas}
\begin{itemize}
    \item \textbf{Lecture 11 Lemma (Lemma 3):} If a time-invariant MC is identically distributed then it is a stationary process
    \item \textbf{Lecture 11 Lemma (Lemma 4):}  If a time-invariant MC has its initial probability distribution \(P_{X_1}\) given by the chain's stationary distribution \(\Pi\), then the MC is a \textit{Stationary Process}
\end{itemize}

\subsection{Entropy Rates and Data Compression (Week 5)}
\subsubsection{Definitions}
\begin{itemize}
    \item \textbf{Lecture 12 Definition (Entropy Rate):} For a source \(\{X_i\}^\infty_{i=1}\) with alphabet \(\mathcal{X}\) the entropy rate is denoted by \(H(\mathcal{X})\) and defined as
    \[H(\mathcal{X}) := \lim_{n \to \infty} \frac{1}{n} H(X_1, \ldots, X_n)\]
    \item \textbf{Lecture 12 Definition (Total redundancy for stationary ergodic source): } Redundancy is the amount of useless information that can be elliminated with fixed-length data compression codes.
    For a stationary ergodic source, its total redundancy \(\rho_T\) is defined as follows
    \[\rho_t := \log_2 |\mathcal{X}| - H(\mathcal{X})\]
    \\ \\
    There are 2 types of redundancies. \(\rho_D\) is the redundancy due to \textit{Source's non-uniform marginal PMF}. \(\rho_M\) is the redundancy due to \textit{Source memory}. The definitions are as follows.

    \begin{itemize}
        \item \(\rho_T = \rho_D + \rho_M\)
        \item \(\rho_D = \log_2 |\mathcal{X}| - H(X_1)\)
        \item \(\rho_M = H(X_1) - H(\mathcal{X})\)
    \end{itemize}
\end{itemize}
\subsubsection{Theorems/Lemmas}
\begin{itemize}
    \item \textbf{Lecture 12 Lemma (Lemma 1): } For a \textit{stationary source} \(\{X_i\}^\infty_{i=1}\), the sequence of conditionary entropies \(\{H(X_i|X^{i-1})_{i=1}^\infty\}\) is decreased in \(i\) and has a limit denoted by
    \[\tilde{H}(\mathcal{X}) := \lim_{i \to \infty} H(X_i | X^{i-1})\]
    \item \textbf{Lecture 12 Lemma (Lemma 2/Cesaro Mean theorem): } If \(a_n \to a\) as \(n \to \infty\) and \(b_n = \frac{1}{n} \sum_{i=1}^{n}a_i\), then \(b_n \to a\) as \(n \to \infty\) % TODO: Why is this useful
    \item \textbf{Lecture 12 Theorem (Entropy rate of stationary sources): } For a \textit{Stationary Source} \(\{X_i\}_{i=1}^\infty\), its entropy rate \(H(\mathcal{X})\) always exists and is equal to \(\tilde{H}(\mathcal{X})\) (See Lecture 12 Lemma 1)

\end{itemize}
\subsection{Lossless Data Compression (Week 5, 6)}
\subsubsection{Definitions}
\begin{itemize}
    \item \textbf{Lecture 13 Definition (Variable length code): } Given a discrete source \(\{X_i\}^\infty_{i=1}\) with alphabet \(\mathcal{X}\) and given a D-ary code alphabet \(B = \{0,1,\ldots, D-1\}\), \(D \geq 2\) fixed integer, a \textit{D-ary n-th order variable-length code (VLC)} for the source is a map
    \[f: \mathcal{X}^n \to B^*\]
    (Maps n-tuples to D-ary code words of variable lengths)
    \\ \\Where  \(B^* = \text{set of all finite-length strings from B}\): (another way of saying this) \\ \(c \in B^* \Leftrightarrow \exists \text{integer } l\geq1 \text{ such that } c \in B^l \)
    \item \textbf{Lecture 13 Definition (Code book): } The codebook \(\rho\) (abuse of notation) of the VLC is the set of all codewords:
    \[\rho = f(\mathcal{X}^n) = \{f(x^n) \in B^* : x^n \in \mathcal{X}^n\}\]
    \item \textbf{Lecture 13 Definition (uniquely decodeable/lossless):} A VLC is \textit{Lossless} if all finite sequences of source n-tuples are mapped \textit{onto} (1 to 1 map) distinct sequences of codewords
    \item \textbf{Lecture 13 Definition (Average code rate): } 
\end{itemize}
\subsubsection{Theorems/Lemmas}


\section{Tutorial Proofs}
\subsection{Week 2 Tutorial}
\begin{itemize}
    \item Given 2 discrete RVs, \(X, Y\) we have that
    \[H(Y|X) = 0 \Longleftrightarrow \text{Y is a function of X}\]
    \item Given RV \(X\) with alphabet \(\mathcal{X}\) and function \(f: x \to \mathbb{R}\)
    \[H(X) \leq H(f(X))\]
\end{itemize}
\subsection{Week 3 Tutorial}

% Random helpful links
% https://math.stackexchange.com/questions/4531722/simplifying-entropy-hx-y-xy
% https://pysdr.org/content/channel_coding.html
% https://math.stackexchange.com/questions/4547648/prove-ab-log-2ab-leq-a-log-2a-b-log-2b


\end{document}
